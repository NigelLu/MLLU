loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/xl3139/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b
Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.16.2",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/xl3139/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
No `resources_per_trial` arg was passed into `hyperparameter_search`. Setting it to a default value of 1 CPU and 1 GPU for each trial.
E0306 13:35:54.841236938    3652 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies
E0306 13:35:54.921451743    3652 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies
E0306 13:35:55.026179975    3652 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies
2022-03-06 13:35:57,687	WARNING bayesopt.py:393 -- BayesOpt does not support specific sampling methods. The Uniform sampler will be dropped.
E0306 13:35:57.702821282    3652 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies
Traceback (most recent call last):
  File "run_hyperparameter_search.py", line 92, in <module>
    n_trials=10
  File "/ext3/miniconda3/envs/nlp/lib/python3.7/site-packages/transformers/trainer.py", line 1847, in hyperparameter_search
    best_run = backend_dict[backend](self, n_trials, direction, **kwargs)
  File "/ext3/miniconda3/envs/nlp/lib/python3.7/site-packages/transformers/integrations.py", line 292, in run_hp_search_ray
    **kwargs,
  File "/ext3/miniconda3/envs/nlp/lib/python3.7/site-packages/ray/tune/tune.py", line 607, in run
    runner.step()
  File "/ext3/miniconda3/envs/nlp/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 662, in step
    next_trial = self._get_next_trial()  # blocking
  File "/ext3/miniconda3/envs/nlp/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 808, in _get_next_trial
    self._update_trial_queue(blocking=wait_for_trial)
  File "/ext3/miniconda3/envs/nlp/lib/python3.7/site-packages/ray/tune/trial_runner.py", line 1283, in _update_trial_queue
    trial = self._search_alg.next_trial()
  File "/ext3/miniconda3/envs/nlp/lib/python3.7/site-packages/ray/tune/suggest/search_generator.py", line 90, in next_trial
    self._experiment.dir_name)
  File "/ext3/miniconda3/envs/nlp/lib/python3.7/site-packages/ray/tune/suggest/search_generator.py", line 97, in create_trial_if_possible
    suggested_config = self.searcher.suggest(trial_id)
  File "/ext3/miniconda3/envs/nlp/lib/python3.7/site-packages/ray/tune/suggest/bayesopt.py", line 245, in suggest
    mode=self._mode))
RuntimeError: Trying to sample a configuration from BayesOptSearch, but the `metric` (None) or `mode` (None) parameters have not been set. Either pass these arguments when instantiating the search algorithm, or pass them to `tune.run()`.
